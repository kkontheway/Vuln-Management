"""Vulnerability service for business logic."""
import json
import logging
from datetime import datetime
from typing import Dict, Optional, List
from database import get_db_connection
from app.utils.formatters import format_datetime_fields
from app.utils.cache import cache_get, cache_set
from app.repositories import vulnerability_repository as vuln_repo
from app.integrations.nvd import fetch_cve_description
from app.services import device_tag_service
from app.constants.database import (
    TABLE_VULNERABILITIES,
    TABLE_CVE_DEVICE_SNAPSHOTS,
    TABLE_VULNERABILITY_SNAPSHOTS
)

logger = logging.getLogger(__name__)

STATISTICS_CACHE_KEY = "stats:overview"
STATISTICS_CACHE_TTL = 300


def get_vulnerabilities(filters=None, page=1, per_page=50, vuln_id=None):
    """Get paginated vulnerability list with filters.
    
    Args:
        filters (dict): Filter conditions
        page (int): Page number
        per_page (int): Items per page
        vuln_id (str): Specific vulnerability ID (optional)
    
    Returns:
        dict: Response with data, total, page, per_page, total_pages
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        records, total = vuln_repo.get_vulnerabilities(
            connection,
            filters=filters,
            page=page,
            per_page=per_page,
            vuln_id=vuln_id
        )
        
        for row in records:
            row['metasploit_detected'] = bool(row.get('metasploit_detected'))
            row['nuclei_detected'] = bool(row.get('nuclei_detected'))
            row['recordfuture_detected'] = bool(row.get('recordfuture_detected'))
            if 'cve_public_exploit' in row:
                row['cve_public_exploit'] = bool(row.get('cve_public_exploit'))
            row['affected_devices'] = row.get('affected_devices', 0) or 0
            row['device_tags'] = _parse_device_tags(row.get('device_tags'))
            format_datetime_fields(row, ['last_seen_timestamp'])
        
        return {
            'data': records,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        }
    finally:
        connection.close()


def get_cve_vulnerability_report_data(cve_id: str, device_limit: Optional[int] = 50):
    """Assemble device-level vulnerability data for CVE reports."""
    if not cve_id:
        raise ValueError("CVE ID is required")

    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")

    try:
        vulnerabilities = vuln_repo.get_vulnerabilities_by_cve(connection, cve_id)
        if not vulnerabilities:
            return None

        description = vulnerabilities[0].get('cve_description')
        if not description:
            fetched_description = fetch_cve_description(cve_id)
            if fetched_description:
                description = fetched_description
                try:
                    vuln_repo.update_cve_description(connection, cve_id, fetched_description)
                except Exception as update_error:
                    logger.warning("Failed to persist fetched description for %s: %s", cve_id, update_error)

        aggregation = _aggregate_device_vulnerabilities(vulnerabilities)
        affected_devices = aggregation['affected_devices']
        if device_limit is not None:
            affected_devices = affected_devices[:device_limit]

        summary_payload = {
            'total_affected_hosts': aggregation['total_devices'],
            'os_distribution': aggregation['os_distribution'],
            'department_distribution': aggregation['dept_distribution'],
            'cvss_score': aggregation['cvss_score'],
            'severity': aggregation['severity']
        }

        return {
            'cve_id': cve_id,
            'summary': summary_payload,
            'software': aggregation['software'],
            'affected_devices': affected_devices,
            'evidence': aggregation['evidence'],
            'remediation': aggregation['remediation'],
            'description': description or None,
            'total_vulnerabilities': aggregation['total_vulnerabilities']
        }
    finally:
        connection.close()


def get_patchthis_vulnerabilities(limit: Optional[int] = None, vendor_scope: Optional[str] = None):
    """Get high-priority vulnerabilities for PatchThis table."""
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")

    try:
        records = vuln_repo.get_patchthis_vulnerabilities(connection, limit=limit, vendor_scope=vendor_scope)
        for row in records:
            row['metasploit_detected'] = bool(row.get('metasploit_detected'))
            row['nuclei_detected'] = bool(row.get('nuclei_detected'))
            row['recordfuture_detected'] = bool(row.get('recordfuture_detected'))
            row['cve_public_exploit'] = bool(row.get('cve_public_exploit'))
            row['affected_devices'] = row.get('affected_devices', 0) or 0
            row['device_tags'] = _parse_device_tags(row.get('device_tags'))
            format_datetime_fields(row, ['last_seen_timestamp'])
        return records
    finally:
        connection.close()


def get_statistics():
    """Get vulnerability statistics for charts.
    
    Returns:
        dict: Statistics by severity, status, platform, vendor, exploitability
    """
    cached = cache_get(STATISTICS_CACHE_KEY)
    if cached:
        return cached
    
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        severity_raw = vuln_repo.get_severity_stats(connection)
        status_raw = vuln_repo.get_status_stats(connection)
        platform_raw = vuln_repo.get_platform_stats(connection)
        vendor_raw = vuln_repo.get_vendor_stats(connection)
        exploitability_raw = vuln_repo.get_exploitability_stats(connection)
        age_distribution_raw = vuln_repo.get_age_distribution(connection)
        exploitability_ratio_raw = vuln_repo.get_exploitability_ratio(connection)
        autopatch_coverage_raw = vuln_repo.get_autopatch_coverage(connection)
        autopatch_epss_raw = vuln_repo.get_autopatch_epss_coverage(connection)
        epss_distribution_raw = vuln_repo.get_epss_distribution(connection)
        intel_overlap_raw = vuln_repo.get_intelligence_feed_overlap(connection)
        new_vulnerabilities_7days = vuln_repo.get_new_vulnerabilities_count(connection, days=7)
        device_tag_distribution_raw = device_tag_service.get_device_tag_distribution(connection)
        
        severity_stats = [{'name': row['vulnerability_severity_level'], 'value': row['count']} for row in severity_raw]
        status_stats = [{'name': row['status'], 'value': row['count']} for row in status_raw]
        platform_stats = [{'name': row['os_platform'], 'value': row['count']} for row in platform_raw]
        vendor_stats = [{'name': row['software_vendor'], 'value': row['count']} for row in vendor_raw]
        exploitability_stats = [{'name': row['exploitability_level'], 'value': row['count']} for row in exploitability_raw]
        
        age_ranges = ['< 30天', '30-60天', '60-90天', '> 90天']
        age_distribution_stats = {
            age_range: {'Critical': 0, 'High': 0, 'Medium': 0, 'Low': 0, 'Other': 0}
            for age_range in age_ranges
        }
        for row in age_distribution_raw:
            age_range = row['age_range']
            severity = row['severity_level']
            if age_range in age_distribution_stats and severity in age_distribution_stats[age_range]:
                age_distribution_stats[age_range][severity] = row['count']
        
        exploitability_ratio_stats = [{'name': row['exploitability_status'], 'value': row['count']} for row in exploitability_ratio_raw]
        
        autopatch_coverage = {
            'critical': {'covered': 0, 'not_covered': 0},
            'high': {'covered': 0, 'not_covered': 0},
            'medium': {'covered': 0, 'not_covered': 0}
        }
        for row in autopatch_coverage_raw:
            severity = row['severity']
            if not severity:
                continue
            severity_key = severity.lower()
            if severity_key in autopatch_coverage:
                bucket = autopatch_coverage[severity_key]
                if row['autopatch_covered']:
                    bucket['covered'] = row['count']
                else:
                    bucket['not_covered'] = row['count']

        epss_coverage_template = {
            'low': {'covered': 0, 'not_covered': 0},
            'medium': {'covered': 0, 'not_covered': 0},
            'high': {'covered': 0, 'not_covered': 0},
            'critical': {'covered': 0, 'not_covered': 0},
        }
        for row in autopatch_epss_raw:
            bucket = (row.get('bucket') or '').lower()
            bucket_entry = epss_coverage_template.get(bucket)
            if not bucket_entry:
                continue
            if row['autopatch_covered']:
                bucket_entry['covered'] = row['count']
            else:
                bucket_entry['not_covered'] = row['count']

        epss_bucket_order = [
            'Low (0-0.5)',
            'Medium (0.5-0.8)',
            'High (0.8-0.90)',
            'Critical (>0.90)'
        ]
        epss_distribution_map = {row['bucket']: row['count'] for row in epss_distribution_raw}
        epss_distribution_stats = [
            {
                'name': bucket,
                'value': epss_distribution_map.get(bucket, 0)
            }
            for bucket in epss_bucket_order
        ]

        feed_overlap_stats = [
            {'name': 'Nuclei Only', 'value': intel_overlap_raw.get('nuclei_only', 0)},
            {'name': 'Metasploit Only', 'value': intel_overlap_raw.get('metasploit_only', 0)},
            {'name': 'RecordFuture Only', 'value': intel_overlap_raw.get('recordfuture_only', 0)},
            {'name': 'Two Feeds', 'value': intel_overlap_raw.get('two_feeds', 0)},
            {'name': 'Three Feeds', 'value': intel_overlap_raw.get('three_feeds', 0)},
        ]

        device_tag_stats = [
            {'name': row['tag'], 'value': row['device_count']}
            for row in device_tag_distribution_raw
            if row.get('tag')
        ]

        stats_payload = {
            'severity': severity_stats,
            'status': status_stats,
            'platform': platform_stats,
            'vendor': vendor_stats,
            'exploitability': exploitability_stats,
            'age_distribution': age_distribution_stats,
            'exploitability_ratio': exploitability_ratio_stats,
            'autopatch_coverage': autopatch_coverage,
            'new_vulnerabilities_7days': new_vulnerabilities_7days,
            'epss_distribution': epss_distribution_stats,
            'intel_feed_overlap': feed_overlap_stats,
            'autopatch_epss_coverage': epss_coverage_template,
            'device_tags': device_tag_stats,
        }
        cache_set(STATISTICS_CACHE_KEY, stats_payload, ttl=STATISTICS_CACHE_TTL)
        return stats_payload
    finally:
        connection.close()


def get_unique_cve_count():
    """Get count of unique CVE IDs.
    
    Returns:
        dict: unique_cve_count
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("Database connection failed")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        # Use vulnerabilities table with DISTINCT for accurate count (deduplicated CVE list)
        query = """
        SELECT COUNT(DISTINCT cve_id) as unique_count
        FROM vulnerabilities
        WHERE cve_id IS NOT NULL AND cve_id != ''
        """
        cursor.execute(query)
        result = cursor.fetchone()
        
        return {
            'unique_cve_count': result['unique_count'] if result else 0
        }
    finally:
        cursor.close()
        connection.close()


def get_severity_counts():
    """Get vulnerability counts by severity level.
    
    Returns:
        dict: Counts by severity (critical, high, medium, low, other)
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("Database connection failed")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        query = """
        SELECT 
            vulnerability_severity_level,
            COUNT(DISTINCT cve_id) as count
        FROM vulnerabilities
        WHERE vulnerability_severity_level IS NOT NULL 
          AND vulnerability_severity_level != ''
          AND cve_id IS NOT NULL
          AND cve_id != ''
        GROUP BY vulnerability_severity_level
        """
        cursor.execute(query)
        results = cursor.fetchall()
        
        # Convert to dictionary format
        severity_counts = {}
        for row in results:
            severity_lower = row['vulnerability_severity_level'].lower()
            if 'critical' in severity_lower:
                severity_counts['critical'] = row['count']
            elif 'high' in severity_lower:
                severity_counts['high'] = row['count']
            elif 'medium' in severity_lower:
                severity_counts['medium'] = row['count']
            elif 'low' in severity_lower:
                severity_counts['low'] = row['count']
            else:
                severity_counts[row['vulnerability_severity_level']] = row['count']
        
        return {
            'critical': severity_counts.get('critical', 0),
            'high': severity_counts.get('high', 0),
            'medium': severity_counts.get('medium', 0),
            'low': severity_counts.get('low', 0),
            'other': sum(v for k, v in severity_counts.items() 
                        if k not in ['critical', 'high', 'medium', 'low'])
        }
    finally:
        cursor.close()
        connection.close()


def get_fixed_vulnerabilities(limit: int = 50):
    """Get list of fixed vulnerabilities (exist in snapshot but not in current vulnerabilities).
    
    Fixed vulnerabilities are CVE-Device combinations that:
    - Exist in the latest snapshot (cve_device_snapshots)
    - Do NOT exist in current vulnerabilities table
    
    Args:
        limit: Maximum number of records to return (default: 50)
    
    Returns:
        list: List of fixed vulnerability records with cve_id, device_name, severity, fixed_date
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        # First, check if any snapshots exist
        cursor.execute(f"SELECT COUNT(*) as count FROM {TABLE_VULNERABILITY_SNAPSHOTS}")
        snapshot_count = cursor.fetchone()['count']
        
        if snapshot_count == 0:
            logger.warning("No snapshots found. Fixed vulnerabilities cannot be determined without snapshots.")
            return []
        
        # Fetch the latest snapshot (most recent run regardless of day)
        cursor.execute(f"""
            SELECT id, snapshot_time
            FROM {TABLE_VULNERABILITY_SNAPSHOTS}
            ORDER BY snapshot_time DESC
            LIMIT 1
        """)
        latest_snapshot = cursor.fetchone()
        if not latest_snapshot:
            logger.warning("Latest snapshot not found.")
            return []

        latest_snapshot_time = latest_snapshot['snapshot_time']
        latest_snapshot_date = latest_snapshot_time.date()

        # Find the last snapshot from any day before the latest snapshot's calendar day
        cursor.execute(
            f"""
            SELECT id, snapshot_time
            FROM {TABLE_VULNERABILITY_SNAPSHOTS}
            WHERE DATE(snapshot_time) < %s
            ORDER BY snapshot_time DESC
            LIMIT 1
            """,
            (latest_snapshot_date,)
        )
        previous_snapshot = cursor.fetchone()
        if not previous_snapshot:
            logger.warning(
                "No prior-day snapshot found before %s; cannot determine fixed vulnerabilities.",
                latest_snapshot_date
            )
            return []
        
        latest_snapshot_id = latest_snapshot['id']
        previous_snapshot_id = previous_snapshot['id']
        logger.info(
            "Using snapshot comparison - current ID: %s (time: %s), previous ID: %s (time: %s)",
            latest_snapshot_id,
            latest_snapshot['snapshot_time'],
            previous_snapshot_id,
            previous_snapshot['snapshot_time']
        )
        
        # Check if previous snapshot has data
        cursor.execute(f"""
            SELECT COUNT(*) as count 
            FROM {TABLE_CVE_DEVICE_SNAPSHOTS} 
            WHERE snapshot_id = %s
        """, (previous_snapshot_id,))
        snapshot_records_count = cursor.fetchone()['count']
        logger.info(f"Found {snapshot_records_count} CVE-Device combinations in previous snapshot")
        
        if snapshot_records_count == 0:
            logger.warning(f"No CVE-Device combinations found in snapshot {previous_snapshot_id}")
            return []
        
        # Get fixed vulnerabilities: exist in previous snapshot but not in current vulnerabilities (latest data)
        fixed_query = f"""
        SELECT 
            cds.cve_id,
            cds.device_name,
            cds.severity,
            %s as fixed_date
        FROM {TABLE_CVE_DEVICE_SNAPSHOTS} cds
        LEFT JOIN (
            SELECT DISTINCT cve_id, device_id
            FROM {TABLE_VULNERABILITIES}
            WHERE cve_id IS NOT NULL AND device_id IS NOT NULL
        ) current_vulns ON cds.cve_id = current_vulns.cve_id 
            AND cds.device_id = current_vulns.device_id
        WHERE cds.snapshot_id = %s
          AND current_vulns.cve_id IS NULL
          AND cds.cve_id IS NOT NULL
          AND cds.cve_id != ''
        ORDER BY cds.cve_id
        LIMIT %s
        """
        cursor.execute(
            fixed_query,
            (latest_snapshot_time, previous_snapshot_id, limit)
        )
        results = cursor.fetchall()
        
        logger.info(f"Found {len(results)} fixed vulnerabilities (limit: {limit})")
        
        # Format datetime fields
        for row in results:
            if row.get('fixed_date'):
                format_datetime_fields(row, ['fixed_date'])
        
        return results
    except Exception as e:
        logger.error(f"Error getting fixed vulnerabilities: {e}", exc_info=True)
        return []
    finally:
        cursor.close()
        connection.close()


def get_catalog_details(cve_id: str) -> Optional[Dict]:
    if not cve_id:
        return None

    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")

    try:
        entry = vuln_repo.get_vulnerability_catalog_entry(connection, cve_id)
        if not entry:
            return None

        description = entry.get('description')
        if not description:
            fetched_description = fetch_cve_description(cve_id)
            if fetched_description:
                entry['description'] = fetched_description
                try:
                    vuln_repo.update_cve_description(connection, cve_id, fetched_description)
                except Exception as update_error:
                    logger.warning("Failed to persist NVD description for %s: %s", cve_id, update_error)

        if 'cve_public_exploit' in entry:
            entry['cve_public_exploit'] = bool(entry.get('cve_public_exploit'))
        format_datetime_fields(entry, ['last_seen_timestamp'])
        devices = vuln_repo.get_devices_for_cve(connection, cve_id)
        for device in devices:
            format_datetime_fields(device, ['last_seen_timestamp'])
        entry['devices'] = devices
        return entry
    finally:
        connection.close()


def _aggregate_device_vulnerabilities(vulnerabilities: List[Dict]) -> Dict:
    """Normalize device-level records and compute aggregation stats."""
    device_map: Dict[str, Dict] = {}
    os_distribution: Dict[str, int] = {}
    dept_distribution: Dict[str, int] = {}

    for vuln in vulnerabilities:
        os_platform = vuln.get('os_platform') or 'Unknown'
        if isinstance(os_platform, str):
            os_platform = os_platform.strip() or 'Unknown'
        dept = vuln.get('rbac_group_name') or 'Unknown'
        if isinstance(dept, str):
            dept = dept.strip() or 'Unknown'

        os_distribution[os_platform] = os_distribution.get(os_platform, 0) + 1
        dept_distribution[dept] = dept_distribution.get(dept, 0) + 1

        device_key = _build_device_key(vuln)
        device_entry = device_map.get(device_key)
        disk_paths = _normalize_path_list(vuln.get('disk_paths'))
        registry_paths = _normalize_path_list(vuln.get('registry_paths'))

        if not device_entry:
            device_entry = {
                'device_id': vuln.get('device_id'),
                'device_name': vuln.get('device_name'),
                'os_platform': vuln.get('os_platform') or 'Unknown',
                'os_version': vuln.get('os_version') or '',
                'rbac_group_name': vuln.get('rbac_group_name') or 'Unknown',
                'status': vuln.get('status') or 'Vulnerable',
                'disk_paths': [],
                'registry_paths': []
            }
            device_map[device_key] = device_entry

        device_entry['disk_paths'] = _merge_unique_lists(device_entry['disk_paths'], disk_paths)
        device_entry['registry_paths'] = _merge_unique_lists(device_entry['registry_paths'], registry_paths)

    affected_devices = list(device_map.values())
    first_vuln = vulnerabilities[0]

    software_info = {
        'vendor': first_vuln.get('software_vendor') or '',
        'name': first_vuln.get('software_name') or '',
        'version': first_vuln.get('software_version') or ''
    }
    remediation = {
        'security_update_available': _coerce_bool(first_vuln.get('security_update_available')),
        'recommended_security_update': first_vuln.get('recommended_security_update') or '',
        'recommended_security_update_id': first_vuln.get('recommended_security_update_id') or '',
        'recommended_security_update_url': first_vuln.get('recommended_security_update_url') or '',
        'recommendation_reference': first_vuln.get('recommendation_reference') or ''
    }

    evidence = {
        'disk_paths': _collect_unique_paths(affected_devices, 'disk_paths', limit=10),
        'registry_paths': _collect_unique_paths(affected_devices, 'registry_paths', limit=10)
    }

    return {
        'affected_devices': affected_devices,
        'total_devices': len(affected_devices),
        'os_distribution': os_distribution,
        'dept_distribution': dept_distribution,
        'severity': first_vuln.get('vulnerability_severity_level') or first_vuln.get('severity'),
        'cvss_score': first_vuln.get('cvss_score'),
        'software': software_info,
        'remediation': remediation,
        'evidence': evidence,
        'total_vulnerabilities': len(vulnerabilities)
    }


def _normalize_path_list(raw_value) -> List[str]:
    """Ensure evidence paths are returned as a clean list of strings."""
    if raw_value is None:
        return []
    if isinstance(raw_value, list):
        return [str(item).strip() for item in raw_value if str(item).strip()]
    if isinstance(raw_value, (bytes, bytearray)):
        raw_value = raw_value.decode('utf-8', errors='ignore')
    if isinstance(raw_value, str):
        trimmed = raw_value.strip()
        if not trimmed:
            return []
        try:
            parsed = json.loads(trimmed)
            if isinstance(parsed, list):
                return [str(item).strip() for item in parsed if str(item).strip()]
        except ValueError:
            return [trimmed]
        return [trimmed]
    return [str(raw_value).strip()]


def _merge_unique_lists(existing: List[str], new_items: List[str]) -> List[str]:
    """Extend list with unique values while preserving order."""
    seen = set(existing)
    for item in new_items:
        if not item:
            continue
        if item not in seen:
            existing.append(item)
            seen.add(item)
    return existing


def _collect_unique_paths(devices: List[Dict], field_name: str, limit: int = 10) -> List[str]:
    """Collect unique evidence paths across devices up to a limit."""
    collected: List[str] = []
    seen = set()
    for device in devices:
        for path in device.get(field_name, []):
            normalized = path.strip() if isinstance(path, str) else str(path)
            if not normalized or normalized in seen:
                continue
            collected.append(normalized)
            seen.add(normalized)
            if limit and len(collected) >= limit:
                return collected
    return collected


def _coerce_bool(value) -> bool:
    """Normalize truthy values."""
    if isinstance(value, bool):
        return value
    if value in (None, ''):
        return False
    if isinstance(value, (int, float)):
        return bool(value)
    if isinstance(value, str):
        return value.strip().lower() in ('1', 'true', 'yes', 'y')
    return bool(value)


def _build_device_key(vuln: Dict) -> str:
    """Generate a stable key for grouping device rows."""
    key = vuln.get('device_id') or vuln.get('device_name')
    if key:
        return str(key)
    return f"unknown-{vuln.get('id') or id(vuln)}"


def _parse_device_tags(raw_value) -> List[str]:
    if not raw_value:
        return []
    if isinstance(raw_value, list):
        return [str(item).strip() for item in raw_value if str(item).strip()]
    return [tag.strip() for tag in str(raw_value).split(',') if tag.strip()]


def get_filter_options():
    """Get filter option lists for dropdowns.
    
    Returns:
        dict: Options for each filter field
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        options = {}
        fields = [
            'vulnerability_severity_level',
            'status',
            'os_platform',
            'exploitability_level',
            'rbac_group_name',
            'software_vendor'
        ]
        
        for field in fields:
            query = f"""
            SELECT DISTINCT {field} as value
            FROM vulnerabilities
            WHERE {field} IS NOT NULL AND {field} != ''
            ORDER BY {field}
            LIMIT 100
            """
            cursor.execute(query)
            options[field] = [row['value'] for row in cursor.fetchall()]
        
        options['device_tag'] = device_tag_service.get_distinct_device_tags(connection)
        
        return options
    finally:
        cursor.close()
        connection.close()
