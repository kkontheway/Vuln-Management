"""Vulnerability service for business logic."""
import logging
from datetime import datetime
from typing import Dict, Optional
from database import get_db_connection
from app.utils.formatters import format_datetime_fields
from app.utils.cache import cache_get, cache_set
from app.repositories import vulnerability_repository as vuln_repo
from app.constants.database import (
    TABLE_VULNERABILITIES,
    TABLE_CVE_DEVICE_SNAPSHOTS,
    TABLE_VULNERABILITY_SNAPSHOTS
)

logger = logging.getLogger(__name__)

STATISTICS_CACHE_KEY = "stats:overview"
STATISTICS_CACHE_TTL = 300


def get_vulnerabilities(filters=None, page=1, per_page=50, vuln_id=None):
    """Get paginated vulnerability list with filters.
    
    Args:
        filters (dict): Filter conditions
        page (int): Page number
        per_page (int): Items per page
        vuln_id (str): Specific vulnerability ID (optional)
    
    Returns:
        dict: Response with data, total, page, per_page, total_pages
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        records, total = vuln_repo.get_vulnerabilities(
            connection,
            filters=filters,
            page=page,
            per_page=per_page,
            vuln_id=vuln_id
        )
        
        for row in records:
            row['metasploit_detected'] = bool(row.get('metasploit_detected'))
            row['nuclei_detected'] = bool(row.get('nuclei_detected'))
            row['recordfuture_detected'] = bool(row.get('recordfuture_detected'))
            row['affected_devices'] = row.get('affected_devices', 0) or 0
            format_datetime_fields(row, ['last_seen_timestamp'])
        
        return {
            'data': records,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        }
    finally:
        connection.close()


def get_statistics():
    """Get vulnerability statistics for charts.
    
    Returns:
        dict: Statistics by severity, status, platform, vendor, exploitability
    """
    cached = cache_get(STATISTICS_CACHE_KEY)
    if cached:
        return cached
    
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        severity_raw = vuln_repo.get_severity_stats(connection)
        status_raw = vuln_repo.get_status_stats(connection)
        platform_raw = vuln_repo.get_platform_stats(connection)
        vendor_raw = vuln_repo.get_vendor_stats(connection)
        exploitability_raw = vuln_repo.get_exploitability_stats(connection)
        age_distribution_raw = vuln_repo.get_age_distribution(connection)
        exploitability_ratio_raw = vuln_repo.get_exploitability_ratio(connection)
        autopatch_coverage_raw = vuln_repo.get_autopatch_coverage(connection)
        epss_distribution_raw = vuln_repo.get_epss_distribution(connection)
        intel_overlap_raw = vuln_repo.get_intelligence_feed_overlap(connection)
        new_vulnerabilities_7days = vuln_repo.get_new_vulnerabilities_count(connection, days=7)
        
        severity_stats = [{'name': row['vulnerability_severity_level'], 'value': row['count']} for row in severity_raw]
        status_stats = [{'name': row['status'], 'value': row['count']} for row in status_raw]
        platform_stats = [{'name': row['os_platform'], 'value': row['count']} for row in platform_raw]
        vendor_stats = [{'name': row['software_vendor'], 'value': row['count']} for row in vendor_raw]
        exploitability_stats = [{'name': row['exploitability_level'], 'value': row['count']} for row in exploitability_raw]
        
        age_ranges = ['< 30天', '30-60天', '60-90天', '> 90天']
        age_distribution_stats = {
            age_range: {'Critical': 0, 'High': 0, 'Medium': 0, 'Low': 0, 'Other': 0}
            for age_range in age_ranges
        }
        for row in age_distribution_raw:
            age_range = row['age_range']
            severity = row['severity_level']
            if age_range in age_distribution_stats and severity in age_distribution_stats[age_range]:
                age_distribution_stats[age_range][severity] = row['count']
        
        exploitability_ratio_stats = [{'name': row['exploitability_status'], 'value': row['count']} for row in exploitability_ratio_raw]
        
        autopatch_coverage = {
            'critical': {'covered': 0, 'not_covered': 0},
            'high': {'covered': 0, 'not_covered': 0},
            'medium': {'covered': 0, 'not_covered': 0}
        }
        for row in autopatch_coverage_raw:
            severity = row['severity']
            if not severity:
                continue
            severity_key = severity.lower()
            if severity_key in autopatch_coverage:
                bucket = autopatch_coverage[severity_key]
                if row['autopatch_covered']:
                    bucket['covered'] = row['count']
                else:
                    bucket['not_covered'] = row['count']

        epss_bucket_order = [
            'Low (0-0.5)',
            'Medium (0.5-0.8)',
            'High (0.8-0.90)',
            'Critical (>0.90)'
        ]
        epss_distribution_map = {row['bucket']: row['count'] for row in epss_distribution_raw}
        epss_distribution_stats = [
            {
                'name': bucket,
                'value': epss_distribution_map.get(bucket, 0)
            }
            for bucket in epss_bucket_order
        ]

        feed_overlap_stats = [
            {'name': 'Nuclei Only', 'value': intel_overlap_raw.get('nuclei_only', 0)},
            {'name': 'Metasploit Only', 'value': intel_overlap_raw.get('metasploit_only', 0)},
            {'name': 'RecordFuture Only', 'value': intel_overlap_raw.get('recordfuture_only', 0)},
            {'name': 'Two Feeds', 'value': intel_overlap_raw.get('two_feeds', 0)},
            {'name': 'Three Feeds', 'value': intel_overlap_raw.get('three_feeds', 0)},
        ]

        stats_payload = {
            'severity': severity_stats,
            'status': status_stats,
            'platform': platform_stats,
            'vendor': vendor_stats,
            'exploitability': exploitability_stats,
            'age_distribution': age_distribution_stats,
            'exploitability_ratio': exploitability_ratio_stats,
            'autopatch_coverage': autopatch_coverage,
            'new_vulnerabilities_7days': new_vulnerabilities_7days,
            'epss_distribution': epss_distribution_stats,
            'intel_feed_overlap': feed_overlap_stats
        }
        cache_set(STATISTICS_CACHE_KEY, stats_payload, ttl=STATISTICS_CACHE_TTL)
        return stats_payload
    finally:
        connection.close()


def get_unique_cve_count():
    """Get count of unique CVE IDs.
    
    Returns:
        dict: unique_cve_count
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("Database connection failed")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        # Use vulnerabilities table with DISTINCT for accurate count (deduplicated CVE list)
        query = """
        SELECT COUNT(DISTINCT cve_id) as unique_count
        FROM vulnerabilities
        WHERE cve_id IS NOT NULL AND cve_id != ''
        """
        cursor.execute(query)
        result = cursor.fetchone()
        
        return {
            'unique_cve_count': result['unique_count'] if result else 0
        }
    finally:
        cursor.close()
        connection.close()


def get_severity_counts():
    """Get vulnerability counts by severity level.
    
    Returns:
        dict: Counts by severity (critical, high, medium, low, other)
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("Database connection failed")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        query = """
        SELECT 
            vulnerability_severity_level,
            COUNT(DISTINCT cve_id) as count
        FROM vulnerabilities
        WHERE vulnerability_severity_level IS NOT NULL 
          AND vulnerability_severity_level != ''
          AND cve_id IS NOT NULL
          AND cve_id != ''
        GROUP BY vulnerability_severity_level
        """
        cursor.execute(query)
        results = cursor.fetchall()
        
        # Convert to dictionary format
        severity_counts = {}
        for row in results:
            severity_lower = row['vulnerability_severity_level'].lower()
            if 'critical' in severity_lower:
                severity_counts['critical'] = row['count']
            elif 'high' in severity_lower:
                severity_counts['high'] = row['count']
            elif 'medium' in severity_lower:
                severity_counts['medium'] = row['count']
            elif 'low' in severity_lower:
                severity_counts['low'] = row['count']
            else:
                severity_counts[row['vulnerability_severity_level']] = row['count']
        
        return {
            'critical': severity_counts.get('critical', 0),
            'high': severity_counts.get('high', 0),
            'medium': severity_counts.get('medium', 0),
            'low': severity_counts.get('low', 0),
            'other': sum(v for k, v in severity_counts.items() 
                        if k not in ['critical', 'high', 'medium', 'low'])
        }
    finally:
        cursor.close()
        connection.close()


def get_fixed_vulnerabilities(limit: int = 50):
    """Get list of fixed vulnerabilities (exist in snapshot but not in current vulnerabilities).
    
    Fixed vulnerabilities are CVE-Device combinations that:
    - Exist in the latest snapshot (cve_device_snapshots)
    - Do NOT exist in current vulnerabilities table
    
    Args:
        limit: Maximum number of records to return (default: 50)
    
    Returns:
        list: List of fixed vulnerability records with cve_id, device_name, severity, fixed_date
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        # First, check if any snapshots exist
        cursor.execute(f"SELECT COUNT(*) as count FROM {TABLE_VULNERABILITY_SNAPSHOTS}")
        snapshot_count = cursor.fetchone()['count']
        
        if snapshot_count == 0:
            logger.warning("No snapshots found. Fixed vulnerabilities cannot be determined without snapshots.")
            return []
        
        # Fetch the latest snapshot (most recent run regardless of day)
        cursor.execute(f"""
            SELECT id, snapshot_time
            FROM {TABLE_VULNERABILITY_SNAPSHOTS}
            ORDER BY snapshot_time DESC
            LIMIT 1
        """)
        latest_snapshot = cursor.fetchone()
        if not latest_snapshot:
            logger.warning("Latest snapshot not found.")
            return []

        latest_snapshot_time = latest_snapshot['snapshot_time']
        latest_snapshot_date = latest_snapshot_time.date()

        # Find the last snapshot from any day before the latest snapshot's calendar day
        cursor.execute(
            f"""
            SELECT id, snapshot_time
            FROM {TABLE_VULNERABILITY_SNAPSHOTS}
            WHERE DATE(snapshot_time) < %s
            ORDER BY snapshot_time DESC
            LIMIT 1
            """,
            (latest_snapshot_date,)
        )
        previous_snapshot = cursor.fetchone()
        if not previous_snapshot:
            logger.warning(
                "No prior-day snapshot found before %s; cannot determine fixed vulnerabilities.",
                latest_snapshot_date
            )
            return []
        
        latest_snapshot_id = latest_snapshot['id']
        previous_snapshot_id = previous_snapshot['id']
        logger.info(
            "Using snapshot comparison - current ID: %s (time: %s), previous ID: %s (time: %s)",
            latest_snapshot_id,
            latest_snapshot['snapshot_time'],
            previous_snapshot_id,
            previous_snapshot['snapshot_time']
        )
        
        # Check if previous snapshot has data
        cursor.execute(f"""
            SELECT COUNT(*) as count 
            FROM {TABLE_CVE_DEVICE_SNAPSHOTS} 
            WHERE snapshot_id = %s
        """, (previous_snapshot_id,))
        snapshot_records_count = cursor.fetchone()['count']
        logger.info(f"Found {snapshot_records_count} CVE-Device combinations in previous snapshot")
        
        if snapshot_records_count == 0:
            logger.warning(f"No CVE-Device combinations found in snapshot {previous_snapshot_id}")
            return []
        
        # Get fixed vulnerabilities: exist in previous snapshot but not in current vulnerabilities (latest data)
        fixed_query = f"""
        SELECT 
            cds.cve_id,
            cds.device_name,
            cds.severity,
            %s as fixed_date
        FROM {TABLE_CVE_DEVICE_SNAPSHOTS} cds
        LEFT JOIN (
            SELECT DISTINCT cve_id, device_id
            FROM {TABLE_VULNERABILITIES}
            WHERE cve_id IS NOT NULL AND device_id IS NOT NULL
        ) current_vulns ON cds.cve_id = current_vulns.cve_id 
            AND cds.device_id = current_vulns.device_id
        WHERE cds.snapshot_id = %s
          AND current_vulns.cve_id IS NULL
          AND cds.cve_id IS NOT NULL
          AND cds.cve_id != ''
        ORDER BY cds.cve_id
        LIMIT %s
        """
        cursor.execute(
            fixed_query,
            (latest_snapshot_time, previous_snapshot_id, limit)
        )
        results = cursor.fetchall()
        
        logger.info(f"Found {len(results)} fixed vulnerabilities (limit: {limit})")
        
        # Format datetime fields
        for row in results:
            if row.get('fixed_date'):
                format_datetime_fields(row, ['fixed_date'])
        
        return results
    except Exception as e:
        logger.error(f"Error getting fixed vulnerabilities: {e}", exc_info=True)
        return []
    finally:
        cursor.close()
        connection.close()


def get_catalog_details(cve_id: str) -> Optional[Dict]:
    if not cve_id:
        return None

    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")

    try:
        entry = vuln_repo.get_vulnerability_catalog_entry(connection, cve_id)
        if not entry:
            return None
        format_datetime_fields(entry, ['last_seen_timestamp'])
        devices = vuln_repo.get_devices_for_cve(connection, cve_id)
        for device in devices:
            format_datetime_fields(device, ['last_seen_timestamp'])
        entry['devices'] = devices
        return entry
    finally:
        connection.close()


def get_filter_options():
    """Get filter option lists for dropdowns.
    
    Returns:
        dict: Options for each filter field
    """
    connection = get_db_connection()
    if not connection:
        raise Exception("数据库连接失败")
    
    try:
        cursor = connection.cursor(dictionary=True)
        
        options = {}
        fields = [
            'vulnerability_severity_level',
            'status',
            'os_platform',
            'exploitability_level',
            'rbac_group_name',
            'software_vendor'
        ]
        
        for field in fields:
            query = f"""
            SELECT DISTINCT {field} as value
            FROM vulnerabilities
            WHERE {field} IS NOT NULL AND {field} != ''
            ORDER BY {field}
            LIMIT 100
            """
            cursor.execute(query)
            options[field] = [row['value'] for row in cursor.fetchall()]
        
        return options
    finally:
        cursor.close()
        connection.close()
